#!/usr/bin/env python3
"""
Production Data Sync Service
–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π –∏ production
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –¥–µ–ø–ª–æ—è—Ö
"""
import asyncio
import os
import sys
import time
import logging
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
os.environ["ENVIRONMENT"] = "production"
os.environ["PYTHONPATH"] = "/app"
os.environ["PYTHONUNBUFFERED"] = "1"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/app/logs/data-sync.log')
    ]
)
logger = logging.getLogger(__name__)

class ProductionDataSync:
    """–°–µ—Ä–≤–∏—Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö production"""
    
    def __init__(self):
        self.backup_dir = Path("/app/data/backups")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self.sync_interval = 300  # 5 –º–∏–Ω—É—Ç
        self.backup_interval = 3600  # 1 —á–∞—Å
        self.last_backup = None
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –±—ç–∫–∞–ø–∞
        self.critical_tables = [
            'users',
            'broadcast_messages',
            'admin_users',
            'audit_logs',
            'message_templates',
            'scheduled_broadcasts'
        ]
    
    async def create_data_backup(self):
        """–°–æ–∑–¥–∞—Ç—å –±—ç–∫–∞–ø –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            from database.db_adapter import DatabaseAdapter
            
            database_url = os.getenv('DATABASE_URL')
            adapter = DatabaseAdapter(database_url)
            await adapter.connect()
            
            backup_data = {
                "timestamp": datetime.now().isoformat(),
                "database_type": adapter.db_type,
                "tables": {}
            }
            
            logger.info("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            
            for table in self.critical_tables:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                    if adapter.db_type == 'sqlite':
                        check_query = "SELECT name FROM sqlite_master WHERE type='table' AND name=?"
                    else:  # PostgreSQL
                        check_query = "SELECT table_name FROM information_schema.tables WHERE table_name = $1"
                    
                    result = await adapter.fetch_one(check_query, (table,))
                    
                    if result:
                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
                        data = await adapter.fetch_all(f"SELECT * FROM {table}")
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
                        table_data = []
                        if data:
                            for row in data:
                                if isinstance(row, dict):
                                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏
                                    row_dict = {}
                                    for key, value in row.items():
                                        if isinstance(value, datetime):
                                            row_dict[key] = value.isoformat()
                                        else:
                                            row_dict[key] = value
                                    table_data.append(row_dict)
                                elif isinstance(row, (tuple, list)):
                                    # –ï—Å–ª–∏ row —ç—Ç–æ tuple/list, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
                                    row_list = []
                                    for value in row:
                                        if isinstance(value, datetime):
                                            row_list.append(value.isoformat())
                                        else:
                                            row_list.append(value)
                                    table_data.append(row_list)
                                else:
                                    table_data.append(str(row))
                        
                        backup_data["tables"][table] = {
                            "count": len(table_data),
                            "data": table_data[:1000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—ç–∫–∞–ø–∞
                        }
                        
                        logger.info(f"‚úÖ {table}: {len(table_data)} –∑–∞–ø–∏—Å–µ–π")
                    else:
                        logger.warning(f"‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ {table} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                        
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –±—ç–∫–∞–ø–∞ —Ç–∞–±–ª–∏—Ü—ã {table}: {e}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±—ç–∫–∞–ø
            backup_file = self.backup_dir / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            backup_file.write_text(json.dumps(backup_data, indent=2, ensure_ascii=False))
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10)
            await self.cleanup_old_backups()
            
            await adapter.disconnect()
            
            self.last_backup = datetime.now()
            logger.info(f"‚úÖ –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {backup_file}")
            
            return backup_file
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞: {e}")
            return None
    
    async def cleanup_old_backups(self):
        """–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –±—ç–∫–∞–ø—ã"""
        try:
            backup_files = sorted(self.backup_dir.glob("backup_*.json"))
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –±—ç–∫–∞–ø–æ–≤
            if len(backup_files) > 10:
                for old_backup in backup_files[:-10]:
                    old_backup.unlink()
                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –±—ç–∫–∞–ø: {old_backup.name}")
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –±—ç–∫–∞–ø–æ–≤: {e}")
    
    async def monitor_data_integrity(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            from database.db_adapter import DatabaseAdapter
            
            database_url = os.getenv('DATABASE_URL')
            adapter = DatabaseAdapter(database_url)
            await adapter.connect()
            
            integrity_report = {
                "timestamp": datetime.now().isoformat(),
                "status": "healthy",
                "issues": []
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            for table in self.critical_tables:
                try:
                    count = await adapter.fetch_one(f"SELECT COUNT(*) FROM {table}")
                    count_value = count[0] if count else 0
                    
                    if table == 'admin_users' and count_value == 0:
                        integrity_report["issues"].append(f"–¢–∞–±–ª–∏—Ü–∞ {table} –ø—É—Å—Ç–∞ - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞!")
                        integrity_report["status"] = "critical"
                    
                    logger.debug(f"üìä {table}: {count_value} –∑–∞–ø–∏—Å–µ–π")
                    
                except Exception as e:
                    integrity_report["issues"].append(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {table}: {str(e)}")
                    integrity_report["status"] = "warning"
            
            await adapter.disconnect()
            
            if integrity_report["status"] != "healthy":
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å—é –¥–∞–Ω–Ω—ã—Ö: {integrity_report['issues']}")
            
            return integrity_report
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {e}")
            return {"status": "error", "error": str(e)}
    
    async def sync_critical_data(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏
            restart_flag = Path("/app/data/restart_required.flag")
            
            if restart_flag.exists():
                logger.info("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–ª–∞–≥ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ - —Å–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –±—ç–∫–∞–ø")
                await self.create_data_backup()
                restart_flag.unlink()  # –£–¥–∞–ª—è–µ–º —Ñ–ª–∞–≥
            
            # –†–µ–≥—É–ª—è—Ä–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            integrity = await self.monitor_data_integrity()
            
            if integrity["status"] == "critical":
                logger.error("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê —Å –¥–∞–Ω–Ω—ã–º–∏!")
                await self.create_data_backup()
            
            # –†–µ–≥—É–ª—è—Ä–Ω—ã–π –±—ç–∫–∞–ø
            if (self.last_backup is None or 
                datetime.now() - self.last_backup > timedelta(seconds=self.backup_interval)):
                await self.create_data_backup()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å–µ—Ä–≤–∏—Å–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ Production Data Sync Service...")
        logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –±—ç–∫–∞–ø–æ–≤: {self.backup_dir}")
        logger.info(f"‚è±Ô∏è –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {self.sync_interval} —Å–µ–∫—É–Ω–¥")
        logger.info(f"üíæ –ò–Ω—Ç–µ—Ä–≤–∞–ª –±—ç–∫–∞–ø–∞: {self.backup_interval} —Å–µ–∫—É–Ω–¥")
        
        # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        await self.sync_critical_data()
        
        while True:
            try:
                await asyncio.sleep(self.sync_interval)
                await self.sync_critical_data()
                
            except KeyboardInterrupt:
                logger.info("üëã –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
                break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
                await asyncio.sleep(self.sync_interval)

if __name__ == "__main__":
    try:
        sync_service = ProductionDataSync()
        asyncio.run(sync_service.run())
    except KeyboardInterrupt:
        logger.info("üëã Data Sync Service –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
